import torch
from torch_geometric import utils
import networkx as nx
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix
from xgboost import XGBClassifier
import numpy as np

def read_from_pyg(path):
    """
    # function to read pygod files, ensure proper labels,
    # and convert to NetworkX
    """
    data = torch.load(path)
    data.y = data.y.bool() * 1
    return utils.convert.to_networkx(data, node_attrs=['y'])


def predict(G, embeddings, return_f1_score=False, return_train_test=False, model='lr'):
    """
    :param G: the networkx graph
    :param embeddings: embeddings created by deepwalk
    :param return_f1_score: return f1 scores (both binary and macro f1),
    select this when you want to keep track of scores
    :param return_train_test: return the train and test labels for later use (keep same test set over all benchmarks)
    :param model: the model to use
    This function runs an ML model on the embeddings generated by deepwalk
    """
    y = list(nx.get_node_attributes(G, 'y').values())

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, shuffle=True, stratify=y)

    # Train logistic regression
    if model == 'lr':
        model = LogisticRegression(class_weight='balanced')
    elif model == 'xgb':
        pos_samples = sum(y)
        neg_samples = len(y) - pos_samples
        model = XGBClassifier(max_depth=10, scale_pos_weight=neg_samples / pos_samples)

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluate predictions
    if return_f1_score:
        binary_f1 = f1_score(y_test, y_pred, average='binary')
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        return binary_f1, macro_f1

    elif return_train_test:
        return y_train, y_test
    else:
        binary_f1 = f1_score(y_test, y_pred, average='binary')
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        print(f'Binary test F1: {binary_f1}')
        print(f'Macro test F1: {macro_f1}')
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(cm)
        disp.plot()


def create_freq_dict(G, num_walks):
    A = nx.adjacency_matrix(G)
    # Normalize the adjacency matrix to get the transition matrix
    P = A.todense()
    # Initialize the probability vector
    p = np.ones(A.shape[0])
    # Performing walking
    result = np.zeros(A.shape[0])
    for i in range(num_walks):
        p = p.dot(P)
        result = np.add(result, p)
    # Creating dict
    freq_dict = {i: float(result[:, i]) for i in range(result.shape[1])}
    return freq_dict
