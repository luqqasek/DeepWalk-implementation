import torch
from torch.utils.data import Dataset
from torch_geometric import utils
import networkx as nx
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix
from xgboost import XGBClassifier
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE


def read_from_pyg(path):
    """
    # function to read pygod files, ensure proper labels,
    # and convert to NetworkX
    """
    data = torch.load(path)
    data.y = data.y.bool() * 1
    return utils.convert.to_networkx(data, node_attrs=['y'])


def predict(G, embeddings, return_f1_score=False, return_train_test=False, model='lr'):
    """
    :param G: the networkx graph
    :param embeddings: embeddings created by deepwalk
    :param return_f1_score: return f1 scores (both binary and macro f1),
    select this when you want to keep track of scores
    :param return_train_test: return the train and test labels for later use (keep same test set over all benchmarks)
    :param model: the model to use
    This function runs an ML model on the embeddings generated by deepwalk
    """
    y = list(nx.get_node_attributes(G, 'y').values())

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, shuffle=True, stratify=y)

    # Train logistic regression
    if model == 'lr':
        model = LogisticRegression(class_weight='balanced')
    elif model == 'xgb':
        pos_samples = sum(y)
        neg_samples = len(y) - pos_samples
        model = XGBClassifier(max_depth=10, scale_pos_weight=neg_samples / pos_samples)

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Evaluate predictions
    if return_f1_score:
        binary_f1 = f1_score(y_test, y_pred, average='binary')
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        return binary_f1, macro_f1

    elif return_train_test:
        return y_train, y_test
    else:
        binary_f1 = f1_score(y_test, y_pred, average='binary')
        macro_f1 = f1_score(y_test, y_pred, average='macro')
        print(f'Binary test F1: {binary_f1}')
        print(f'Macro test F1: {macro_f1}')
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(cm)
        disp.plot()


def create_freq_dict(G, num_walks):
    A = nx.adjacency_matrix(G)
    # Normalize the adjacency matrix to get the transition matrix
    P = A.todense()
    # Initialize the probability vector
    p = np.ones(A.shape[0])
    # Performing walking
    result = np.zeros(A.shape[0])
    for i in range(num_walks):
        p = p.dot(P)
        result = np.add(result, p)
    # Creating dict
    freq_dict = {i: float(result[:, i]) for i in range(result.shape[1])}
    return freq_dict


class FraudDataset(Dataset):

    def __init__(self, walks, window_size):

        """
        :param walks: the generated random walks
        :param window_size: the window size for which we will include context words
        """

        self.walks = walks
        self.window_size = window_size

        self.vocab = self.build_vocab()
        # all pairs of (center word,context word)
        self.pairs = self.get_pairs()
        self.X = np.zeros((len(self.pairs), len(self.vocab)))
        self.Y = np.zeros((len(self.pairs), len(self.vocab)))
        self.X[np.arange(len(self.pairs)), [pair[0] for pair in self.pairs]] = 1
        self.Y[np.arange(len(self.pairs)), [pair[1] for pair in self.pairs]] = 1

    def build_vocab(self):
        vocab = list(set(node for walk in self.walks for node in walk))
        return vocab

    def get_pairs(self):
        """
        Giving a list of pairs of the form [(center_word,context_word)]
        From these pairs, we will have to make one-hot encoded vectors
        """
        pairs = []
        for walk in self.walks:
            for center_word_pos in range(len(walk)):
                for w in range(-self.window_size, self.window_size + 1):
                    # if window size is 2, go to -2,-1,0,1,2
                    context_word_pos = center_word_pos + w
                    if context_word_pos < 0 or context_word_pos >= len(walk) or context_word_pos == center_word_pos:
                        continue
                    pairs.append((walk[center_word_pos], walk[context_word_pos]))
        return pairs

    def __getitem__(self, index):
        return self.X[index, :], self.Y[index, :]

    def __len__(self):
        # return the number of samples
        return self.X.shape[0]


def visualize(graph, node2embedding, labeled=False):
    # Get the node list of the graph
    nodelist = list(graph.nodes())

    # Embedding vectors
    x = np.asarray([node2embedding[node] for node in nodelist])
    x_embedded = TSNE(n_components=2).fit_transform(x)
    print(f'TSNE succesfully applied: x now of shape: {x_embedded.shape}')
    df = pd.DataFrame({"Axis 1": x_embedded[:, 0], "Axis 2": x_embedded[:, 1]})

    # Probably a better way to do this, but allows us to toggle class labels
    if not labeled:
        ax = sns.scatterplot(
            x="Axis 1", y="Axis 2",
            data=df,
            alpha=0.8
        )
    else:
        y = list(nx.get_node_attributes(graph, 'y').values())
        ax = sns.scatterplot(
            x="Axis 1", y="Axis 2",
            data=df,
            alpha=0.8,
            hue=y
        )

    plt.figure(figsize=(10, 10))
